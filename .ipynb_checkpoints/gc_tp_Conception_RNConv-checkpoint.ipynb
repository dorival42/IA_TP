{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - Intelligence artificielle pour la santé\n",
    "# Conception de Réseau de neurones convolutif (ConvNet)\n",
    "\n",
    "Gilles Cohen | gilles.cohen@univ.lyon1.fr\n",
    "\n",
    "M2 - UCBL Université Lyon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, on va concevoir notre propre réseau convolutif (**Conv**olutional **Net**work) et voir quelques applications existantes.\n",
    "\n",
    "On verra aussi les trois méthodes différentes pour définir un modèle Keras :\n",
    "\n",
    "- Séquentiel\n",
    "- Fonctionnel\n",
    "- Orienté objet\n",
    "\n",
    "Le but de ce TP n'est pas de comparer les performances des modèles, car nous sommes limités en puissance de calcul, mais de comparer les architectures des modèles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nom étudiant: \n",
    "\n",
    "# Prénom étudiant: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Instruction de soumission\n",
    "1.  L'étudiant doit insérer le code Python où c'est demandé TODO .\n",
    "\n",
    "2.  Préfixer ce fichier notebook avec nom_prenom_\n",
    "\n",
    "3.  Insérez votre nom et prénom dans la cellule suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((50000, 28, 28, 1), (50000,)),\n",
       " ((10000, 28, 28, 1), (10000,)),\n",
       " ((10000, 28, 28, 1), (10000,)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "h, w = x_train.shape[1:]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], h, w, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], h, w, 1)\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=10000, random_state=42)\n",
    "\n",
    "(x_train.shape, y_train.shape), (x_val.shape, y_val.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].squeeze(-1))\n",
    "plt.title(y_train[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] unique labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"{} unique labels.\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. LeNet\n",
    "\n",
    "Définissons un modèle LeNet (légèrement modifié) introduit par Yann Le Cun en 1998 ([paper url](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). Le modèle est très simple et peut être défini avec l'API **Sequential**.\n",
    "\n",
    "![lenet archi](images/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LeNet-5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LeNet-5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ C1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ S2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ C3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ S4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ C5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ F6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ C1 (\u001b[38;5;33mConv2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m156\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ S2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ C3 (\u001b[38;5;33mConv2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ S4 (\u001b[38;5;33mAveragePooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ C5 (\u001b[38;5;33mConv2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m120\u001b[0m)      │        \u001b[38;5;34m48,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ F6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import AvgPool2D, Conv2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "lenet = Sequential(name=\"LeNet-5\")\n",
    "\n",
    "lenet.add(Conv2D(6, kernel_size=(5, 5), activation=\"tanh\", padding=\"same\",\n",
    "                 input_shape=input_shape, name=\"C1\"))\n",
    "lenet.add(MaxPool2D(pool_size=(2, 2), name=\"S2\"))\n",
    "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', name=\"C3\"))\n",
    "lenet.add(AvgPool2D(pool_size=(2, 2), name=\"S4\"))\n",
    "lenet.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', name=\"C5\"))\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(84, activation='tanh', name=\"F6\"))\n",
    "lenet.add(Dense(10, activation='softmax'))\n",
    "\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9709 - loss: 0.1007 - val_accuracy: 0.9584 - val_loss: 0.1273\n",
      "Epoch 2/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9737 - loss: 0.0889 - val_accuracy: 0.9772 - val_loss: 0.0791\n",
      "Epoch 3/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.9791 - loss: 0.0729 - val_accuracy: 0.9748 - val_loss: 0.0852\n",
      "Epoch 4/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9796 - loss: 0.0680 - val_accuracy: 0.9791 - val_loss: 0.0693\n",
      "Epoch 5/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.9816 - loss: 0.0637 - val_accuracy: 0.9820 - val_loss: 0.0602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f06cba16a70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "lenet.compile(\n",
    "    optimizer=optimizers.SGD(learning_rate=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05468202382326126, 0.9819999933242798]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On notera que si LeNet a d'abord été défini en utilisant `tanh` ou `sigmoid`, ces activations sont aujourd'hui rarement utilisées. Ces deux activations saturent pour des valeurs très petites et très grandes, ce qui rend leur gradient presque nul.\n",
    "\n",
    "Aujourd'hui, la plupart des réseaux utilisent `ReLU` comme fonction d'activation cachée ou l'une de ses variations (https://keras.io/layers/advanced-activations/).\n",
    "\n",
    "## 2. Inception\n",
    "\n",
    "Les modèles inception ont été introduits en 2014 par Szegedy et al. ([paper url](https://arxiv.org/abs/1409.4842)).\n",
    "\n",
    "Les convolutions ont un champ réceptif efficace : plus les noyaux sont grands et plus le modèle est profond, plus un pixel de caractéristiques verra de pixels de l'image. Lisez ceci pour une bonne explication : [medium blog](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n",
    "\n",
    "Dans Inception, des noyaux de convolution de différentes tailles sont combinés. Les petits noyaux voient de petits groupes de caractéristiques (pensez à un détail comme un œil) tandis que les gros noyaux voient de grands groupes de caractéristiques (pensez à un visage).\n",
    "\n",
    "![inception archi](images/inception.png)\n",
    "\n",
    "Cette fois, utilisez l'API **Fonctionnelle** pour définir une seule couche d'Inception comme dans l'image précédente.\n",
    "Exemple d'utilisation :\n",
    "\n",
    "```python\n",
    "a = Input(shape=(32,))\n",
    "b = Dense(32)(a)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "```\n",
    "\n",
    "La couche est d'abord instanciée (première paire de parenthèses) puis appelée sur un tenseur (deuxième paire de parenthèses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inputs to a layer should be tensors. Got 'None' (of type <class 'NoneType'>) as input for layer 'flatten_3'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(input_tensor)\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m inception_layer(x, \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[1;32m     23\u001b[0m mini_inception \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_tensor, outputs\u001b[38;5;241m=\u001b[39moutput_tensor)\n",
      "File \u001b[0;32m~/mydata/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mydata/lib/python3.10/site-packages/keras/src/layers/input_spec.py:176\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs to a layer should be tensors. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) as input for layer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    181\u001b[0m shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    182\u001b[0m ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "\u001b[0;31mValueError\u001b[0m: Inputs to a layer should be tensors. Got 'None' (of type <class 'NoneType'>) as input for layer 'flatten_3'."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def inception_layer(tensor, n_filters):\n",
    "    # TODO : définir les 4 branches\n",
    "    branch1x1 = None\n",
    "    branch5x5 = None\n",
    "    branch3x3 = None\n",
    "    branch_pool = None\n",
    "\n",
    "    # TODO : fusionner en utilisant la couche Concatenate, utiliser Concatenate ? pour plus d'informations\n",
    "    output = None\n",
    "    return output\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=input_shape)\n",
    "x = Conv2D(16, kernel_size=(5, 5), padding=\"same\")(input_tensor)\n",
    "x = inception_layer(x, 32)\n",
    "x = Flatten()(x)\n",
    "output_tensor = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mini_inception = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "mini_inception.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet\n",
    "\n",
    "Les modèles ResNet (*Réseaux résiduels*) ont été introduits par He et al. en 2015 ([paper url](https://arxiv.org/abs/1512.03385)). Ils ont trouvé que plus de couches amélioraient la performance mais malheureusement il était difficile de rétropropager les gradients jusqu'aux premières couches.\n",
    "\n",
    "Une astuce pour laisser les gradients \"*couler*\" facilement est d'utiliser une connexion raccourcie qui laisse le tenseur avant intact (alias un *résidu*) :\n",
    "\n",
    "![resnet archi](images/resnet.png)\n",
    "\n",
    "Cette fois, codez une couche ResNet en utilisant l'API **Objet Orienté** :\n",
    "\n",
    "Exemple d'utilisation :\n",
    "``python\n",
    "class MyModel(Model) :\n",
    "    def __init__(self) :\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs) :\n",
    "        return self.classifier(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mini_res_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " ResidualBlock (ResidualBloc  multiple                 0 (unused)\n",
      " k)                                                              \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer, Add\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(name=\"ResidualBlock\")\n",
    "        \n",
    "        # TODO: define needed layers, use Add to combine the shortcut with the convs' output.\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        return 42\n",
    "    \n",
    "\n",
    "class MiniResNet(Model):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(n_filters, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.block = ResidualBlock(n_filters)\n",
    "        self.flatten = Flatten()\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        return 1337\n",
    "\n",
    "\n",
    "mini_resnet = MiniResNet(32)\n",
    "mini_resnet.build((None, *input_shape))\n",
    "mini_resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalisation des lots (Batch Normalization)\n",
    "\n",
    "La normalisation par lots n'est pas une architecture mais une couche. Introduite par Ioffe et al. en 2015 ([paper url](https://arxiv.org/abs/1502.03167)). Voici un extrait de leur résumé :\n",
    "\n",
    "> L'apprentissage des réseaux neuronaux profonds est compliqué par le fait que la **distribution des entrées de chaque couche change pendant l'apprentissage, car les paramètres des couches précédentes changent**. Cela ralentit l'apprentissage en exigeant des taux d'apprentissage plus faibles et une initialisation minutieuse des paramètres, et rend notoirement difficile l'apprentissage de modèles avec des non-linéarités saturantes.  Nous qualifions ce phénomène de **décalage interne des covariables**, et abordons le problème en **normalisant les entrées des couches**.\n",
    "\n",
    "Le résultat est que les ConvNet formés avec BatchNorm convergent plus rapidement et avec de meilleurs résultats. De nos jours, tous les réseaux (ou presque) utilisent cette méthode ou l'une de ses variantes. Voir cet [article sur la normalisation] (https://arthurdouillard.com/post/normalization/) pour plus d'informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un bloc classique est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Layer):\n",
    "    def __init__(n_filters, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = Conv2D(n_filters, kernel_size=kernel_size, use_bias=False)\n",
    "        self.bn = BatchNormalization(axis=3)\n",
    "        self.activation = Activation(\"relu\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activation(\n",
    "            self.bn(self.conv(inputs))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que vous pouvez placer plusieurs fois dans votre réseau comme des blocs de Lego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutions séparables\n",
    "\n",
    "Les ConvNet ont généralement beaucoup de paramètres en raison de leur grande profondeur. Une astuce pour réduire le nombre de paramètres avec une perte de performance minimale est d'utiliser la **convolution séparable**.\n",
    "\n",
    "La convolution standard a beaucoup de paramètres (mais toujours beaucoup moins qu'une couche entièrement connectée !):\n",
    "\n",
    "![conv](images/conv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(name=\"Conv Model\")\n",
    "conv_model.add(Conv2D(8, kernel_size=(3, 3), use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice: \n",
    "\n",
    "- Combien de paramètres comporte cette convolution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les convolutions séparables sont constituées de deux convolutions :\n",
    "\n",
    "- Une **convolution en profondeur**, un seul noyau est créé par canal d'entrée, l'information spatiale est affectée, mais l'information sur les canaux n'est pas partagée.\n",
    "\n",
    "![convolution par profondeur](images/depthwise.png)\n",
    "\n",
    "- Une convolution **pointwise**, est une convolution habituelle avec un noyau de taille (1, 1). L'information spatiale n'est pas affectée, mais l'information sur les canaux est partagée.\n",
    "\n",
    "![pointwise conv](images/pointwise.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import DepthwiseConv2D\n",
    "\n",
    "separable_model = Sequential(name=\"Separable Model\")\n",
    "separable_model.add(DepthwiseConv2D(kernel_size=(3, 3), use_bias=False))\n",
    "separable_model.add(Conv2D(8, kernel_size=(1, 1), use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice:\n",
    "\n",
    "- Combien de paramètres la convolution en profondeur possède-t-elle ?\n",
    "- Combien de paramètres possède la convolution ponctuelle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire\n",
    "\n",
    "- Voir les différents modèles disponibles sur Keras : https://keras.io/applications/ Quelles sont leurs différentes astuces d'architecture ?\n",
    "- Essayez de choisir une architecture (comme [MobileNet](https://arxiv.org/abs/1704.04861) ou [Squeeze-and-Excitation network](https://arxiv.org/abs/1709.01507)), lisez leur article, implémentez-la dans Keras, et essayez d'atteindre de bonnes performances sur un petit jeu de données comme CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
